# Copyright (c) 2023-2025 Antmicro <www.antmicro.com>
#
# SPDX-License-Identifier: Apache-2.0

config KENNING_INFERENCE_LIB
        bool "kenning_inference_lib Support"
        help
          This option enables the kenning_inference_lib library

config KENNING_PYTHON_PATH
        string "path to the model that would be used"
        depends on KENNING_INFERENCE_LIB
        default "python3"
        help
          The path to a Python3 binary, by default a python3
          from current environment is used.

config KENNING_MODEL_PATH
        string "path to the model that would be used"
        depends on KENNING_INFERENCE_LIB
        help
          This options selects the model that would be used by backend that
          require model during compilation (i.e. TVM). Can be path or URI to
          Kenning resource (i.e. kenning:///models/classification/magic_wand.h5)

config KENNING_MODEL_COMPILER_ARGS
        string "additional args passed to Kenning model compiler"
        depends on KENNING_INFERENCE_LIB

config KENNING_NRF_UART_SLEEP_AFTER_POLL_WORKAROUND
        bool "workaround for UART reading on NRF"
        depends on KENNING_INFERENCE_LIB
        default 0
        help
          This option adds 1 tick sleep after uart_poll_in. This is a temporary
          solution for UART missing some data when reading.

choice KENNING_COMMUNICATION_PROTOCOL
        prompt "Protocol to be used to communicate with Kenning"
        depends on KENNING_INFERENCE_LIB
        default KENNING_COMMUNICATION_PROTOCOL_UART
        help
          This options selects protocol which will be used to communicate with
          Kenning
          Available options: PROTOCOL_NONE, PROTOCOL_UART

config KENNING_COMMUNICATION_PROTOCOL_NONE
        bool
        prompt "Communication with Kenning disabled"

config KENNING_COMMUNICATION_PROTOCOL_UART
        bool
        prompt "UART protocol"
        select UART_SIFIVE_PORT_1 if UART_SIFIVE

endchoice

config KENNING_SEND_LOGS
        bool
        prompt "Module for sending logs back do the Kenning client."
        depends on LOG_MODE_IMMEDIATE=y
        depends on KENNING_COMMUNICATION_PROTOCOL_NONE=n
        select LOG_OUTPUT
        select LOG_BACKEND_SUPPORTS_FORMAT_TIMESTAMP
        default 1

config KENNING_LOG_BUFFER_SIZE
        int "Size in bytes of a buffer used to store log messages, before sending them to the Kenning client."
        depends on KENNING_SEND_LOGS
        default 512

config KENNING_RESPONSE_PAYLOAD_SIZE
        int "Size in bytes of the response payload"
        depends on KENNING_INFERENCE_LIB
        default 1024

config KENNING_MESSAGE_RECV_BUFFER_SIZE
        int "Size in bytes of the buffer used for holding message chunks"
        depends on KENNING_INFERENCE_LIB
        default 128

config KENNING_PROTOCOL_MAX_OUTGOING_MESSAGE_SIZE
        int "Maximum size of message payload in bytes."
        depends on KENNING_INFERENCE_LIB
        default 1048576

config KENNING_INCREASE_MEMORY
        bool "Whether board memory should be increased (works only in Renode simulation)"
        default 0

config KENNING_INCREASE_MEMORY_SIZE
        int "Size of increased board memory"
        default 4096

choice KENNING_ML_RUNTIME
        prompt "ML runtime to be used"
        depends on KENNING_INFERENCE_LIB
        default KENNING_ML_RUNTIME_STUB
        help
          This option selects ML runtime to be used.
          Available options: KENNING_ML_RUNTIME_STUB, KENNING_ML_RUNTIME_TVM, KENNING_ML_RUNTIME_TFLITE

config KENNING_ML_RUNTIME_STUB
        bool
        prompt "Runtime stub"

config KENNING_ML_RUNTIME_TVM
        bool
        prompt "microTVM runtime"
        select ZEPHYR_TVM_MODULE
        select ZEPHYR_DLPACK_MODULE
        select CMSIS_NN if CPU_CORTEX_M
        select CMSIS_DSP if CPU_CORTEX_M

config KENNING_ML_RUNTIME_TFLITE
        bool
        prompt "TFLite Micro runtime"
        select TENSORFLOW_LITE_MICRO
        select TENSORFLOW_LITE_MICRO_CMSIS_NN_KERNELS if CPU_CORTEX_M
        select CMSIS_NN if CPU_CORTEX_M
        select CMSIS_DSP if CPU_CORTEX_M

config KENNING_ML_RUNTIME_IREE
        bool
        prompt "IREE runtime"
        select ZEPHYR_IREE_MODULE

config KENNING_IREE_MODEL_BUFFER_SIZE
        int "Size in kilobytes of the IREE model buffer"
        default 32
        depends on KENNING_ML_RUNTIME_IREE

config KENNING_IREE_INPUT_BUFFER_SIZE
        int "Size in kilobytes of the IREE input buffer"
        default 4
        depends on KENNING_ML_RUNTIME_IREE

config KENNING_ML_RUNTIME_AI8X
        bool
        prompt "AI8X runtime"

config KENNING_ML_RUNTIME_LLEXT
        bool
        prompt "Loads Kenning Runtime from Linkable Loadable Extensions."
        select LLEXT

endchoice

choice KENNING_TVM_MODEL
        prompt "TVM model to be loaded"
        default KENNING_TVM_MODEL_GEN if KENNING_MODEL_PATH!=""
        default KENNING_TVM_MODEL_MAGIC_WAND
        depends on KENNING_ML_RUNTIME_TVM || KENNING_ML_RUNTIME_LLEXT
        help
          This option select the ML model to be loaded by TVM runtime.
          Available options: KENNING_TVM_MODEL_MAGIC_WAND, KENNING_TVM_MODE_MAGIC_WAND_INT8, KENNING_TVM_MODEL_GEN

config KENNING_TVM_MODEL_MAGIC_WAND
        bool
        prompt "Magic Wand model"

config KENNING_TVM_MODEL_MAGIC_WAND_INT8
        bool
        prompt "Quantized Magic Wand model"

config KENNING_TVM_MODEL_PRE_GEN
        bool
        prompt "Load model from pre-generated source"

config KENNING_TVM_MODEL_GEN
        bool
        prompt "Load model from generated source"

endchoice

config KENNING_TVM_TARGET_ATTRS
        string "Attributes of TVM target"
        default ""
        depends on KENNING_ML_RUNTIME_TVM || KENNING_ML_RUNTIME_LLEXT

config KENNING_TVM_HEAP_SIZE
        int "Size in kilobytes of the heap used by TVM"
        default 128
        depends on KENNING_ML_RUNTIME_TVM || KENNING_ML_RUNTIME_LLEXT
        help
          This option sets the size in kilobytes of the heap used by TVM.

config KENNING_TVM_GRAPH_BUFFER_SIZE
        int "Size in kilobytes of the TVM graph buffer"
        default 32
        depends on KENNING_ML_RUNTIME_TVM || KENNING_ML_RUNTIME_LLEXT

config KENNING_TVM_INPUT_BUFFER_SIZE
        int "Size in kilobytes of the TVM input buffer"
        default 4
        depends on KENNING_ML_RUNTIME_TVM || KENNING_ML_RUNTIME_LLEXT

config KENNING_TFLITE_BUFFER_SIZE
        int "Size in kilobytes of the buffer for tensor arena and model used by TFLite"
        default 64
        depends on KENNING_ML_RUNTIME_TFLITE || KENNING_ML_RUNTIME_LLEXT
        help
          This option sets the size in bytes of the buffer for tensor arena and model used by TFLite.

config KENNING_TFLITE_OPS
        string "Names of operators to be added to TFLite Micro runtime."
        default ""
        depends on KENNING_ML_RUNTIME_TFLITE
        help
          This option provides op names to include in the TFLite Micro resolver.
          If only KENNING_MODEL_PATH is provided, ops are retrieved from the TFLite model
          provided in the path.
          If only KENNING_TFLITE_OPS is provided, ops from this option are used.
          If both settings are provided, ops from model and KENNING_TFLITE_OPS are used.
          If none of those two options are provided, default set of ops is used:

          Conv2D,FullyConnected,MaxPool2D,Reshape,Softmax

          Ops in KENNING_TFLITE_OPS should be provided as comma-separated list of ops' names, as above.

choice KENNING_IREE_LOADER
        prompt "loader to be used by IREE"
        depends on KENNING_ML_RUNTIME_IREE
        default KENNING_IREE_LOADER_VMVX
        help
          This option selects the loader to be used by IREE.
          Available options: KENNING_IREE_LOADER_VMVX, KENNING_IREE_LOADER_EMBEDDED_ELF

config KENNING_IREE_LOADER_VMVX
        bool
        prompt "vmvx module loader"

config KENNING_IREE_LOADER_EMBEDDED_ELF
        bool
        prompt "embedded elf loader"

endchoice

config KENNING_AI8X_CNN_TIMEOUT_MS
        int "CNN accelerator timeout"
        depends on KENNING_ML_RUNTIME_AI8X
        default 1000

module = CALLBACKS
module-str = callbacks
source "subsys/logging/Kconfig.template.log_config"

module = INFERENCE_SERVER
module-str = inference_server
source "subsys/logging/Kconfig.template.log_config"

module = KENNING_PROTOCOL
module-str = kenning_protocol
source "subsys/logging/Kconfig.template.log_config"

module = MODEL
module-str = model
source "subsys/logging/Kconfig.template.log_config"

module = RUNTIME_WRAPPER
module-str = runtime_wrapper
source "subsys/logging/Kconfig.template.log_config"

module = UART
module-str = uart
source "subsys/logging/Kconfig.template.log_config"

module = LOGGER
module-str = logger
source "subsys/logging/Kconfig.template.log_config"
